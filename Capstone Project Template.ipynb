{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project will answer questins related to US immigration.\n",
    "Data sources were provided by Udacity which are:\n",
    "- 94 Immigration Data\n",
    "- U.S. City Demographic Data\n",
    "- Airport Code Table\n",
    "\n",
    "Technologies used are Spark for processing .SAS files.\n",
    "Pandas for processing .csv files\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of this project is to create Dimention and Fact tables from raw data provided from 3 different sources.\n",
    "The final fact table will show the number of immigrants per country and from whcich airport they came and to which state they went.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "- **I94 Immigrantion Data**: comes form the US National Tourism and Trade Office.\n",
    "- **U.S City Demographic Data**: comes from OpenSoft.\n",
    "- **Airport Code Table: comes** from DataHub.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "* Immigration:\n",
    "    - Read files per month into a Spark DataFrame.\n",
    "    - Filter the missing values.\n",
    "    - Generate the corresponding names and codes per state and city using UDF.\n",
    "    - Write the Dimesion Table into Parquet.\n",
    "\n",
    "* Demographics:\n",
    "    - Group race values into shorter values.\n",
    "    - Calculate the average and sum per State\n",
    "    - Calculate percentages per columns.\n",
    "    - Calculate percentages per race.\n",
    "    - Write the Dimension Table into a CSV file.\n",
    "    \n",
    "* Airports:\n",
    "    - Filter USA data.\n",
    "    - Transform data types for several columns.\n",
    "    - Write the Dimension Table into a CSV file.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The chosen schema for modeling is Star Schema.\n",
    "\n",
    "**Star Schema**\n",
    "\n",
    "1. Dimension Tables\n",
    "    * Immigration\n",
    "    * Demographics\n",
    "    * Airports\n",
    "2. Fact Table\n",
    "    * Fact_Immigration\n",
    "\n",
    "<img src=\"img.png\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "    1. Dimension tables will be populated with the raw data after cleaning.\n",
    "    2. A TempView will be created for every table using Spaark.\n",
    "    3. Spark SQL will be used to create the Fact Table.\n",
    "    4. Fact table generated will be written in a parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Note: Run etl.py before running below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreashold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read every Dimension table\n",
    "demographicsDF = spark.read.csv(\"us_demographics.csv\",header=True)\n",
    "airportsDF = spark.read.csv(\"us_airports.csv\",header=True)\n",
    "immigrationDF = spark.read.parquet(\"immigration_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For time processing purposes, I will be using only the first 2 months in the immigration dataframe\n",
    "immigrationDF = immigrationDF.filter(f.col(\"month\")<f.lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Dimension TempViews\n",
    "immigrationDF.createOrReplaceTempView(\"immigration\")\n",
    "demographicsDF.createOrReplaceTempView(\"demographics\")\n",
    "airportsDF.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Running SQL Procedure to get the fact table dataframe\n",
    "fact_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        immigration.year,\n",
    "        immigration.month,\n",
    "        immigration.origin_country,\n",
    "        immigration.State,\n",
    "        immigration.state_code,\n",
    "        COUNT(immigration.state_code) as number_immigrants,\n",
    "        demographics.median_age,\n",
    "        demographics.percentage_male,\n",
    "        demographics.percentage_female,\n",
    "        demographics.percentage_veterans,\n",
    "        demographics.percentage_foreign_born,\n",
    "        demographics.Afroamerican as percentage_afroamerican,\n",
    "        demographics.Asian as percentage_asian,\n",
    "        demographics.Latino as percentage_latino,\n",
    "        demographics.Native as percentage_native,\n",
    "        demographics.White as percentage_white,\n",
    "        airports.name as airport_name,\n",
    "        airports.x_coordinate,\n",
    "        airports.y_coordinate\n",
    "    FROM immigration\n",
    "    JOIN demographics ON demographics.state_code=immigration.state_code\n",
    "    JOIN airports ON airports.state_code=immigration.state_code\n",
    "    WHERE airports.type='large_airport'\n",
    "    GROUP BY 1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19\n",
    "    ORDER BY 1,2,3,4\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Count rows in dataframe\n",
    "fact_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Fact Table in Parquet format\n",
    "fact_table.write.mode('overwrite').parquet(\"fact_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "fact_table = spark.read.parquet(\"fact_table\")\n",
    "fact_table.createOrReplaceTempView(\"fact_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check null values per column\n",
    "null_values_test = fact_table.select(f.isnull('year'),\\\n",
    "                  f.isnull('month'),\\\n",
    "                  f.isnull('origin_country'),\\\n",
    "                  f.isnull('state_code'),\\\n",
    "                  f.isnull('State')).dropDuplicates()\n",
    "null_values_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if there are immigrants that were no copied from the original immigrants dimensional table\n",
    "immigrants_check_integrity = spark.sql(\"\"\"\n",
    "    SELECT I.year, I.month, I.State, I.state_code\n",
    "    FROM (\n",
    "        SELECT year, month, State, state_code, COUNT(state_code) as number_immigrants\n",
    "        FROM immigration\n",
    "        GROUP BY 1,2,3,4\n",
    "        ORDER BY 1,2,3,4\n",
    "    ) I, \n",
    "    (\n",
    "        SELECT year, month, State, state_code, SUM(number_immigrants) as sum_immigrants\n",
    "        FROM fact_table\n",
    "        GROUP BY 1,2,3,4\n",
    "        ORDER BY 1,2,3,4\n",
    "    ) F\n",
    "    WHERE I.year=F.year and I.month=F.month and I.state_code=F.state_code and I.number_immigrants>F.sum_immigrants\n",
    "\"\"\")\n",
    "immigrants_check_integrity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check Sum for number of immigrants equal to 0\n",
    "immigrants_check_sum = spark.sql(\"\"\"\n",
    "    SELECT A.State, A.state_code, A.sum_immigrants\n",
    "    FROM (\n",
    "        SELECT State, state_code, SUM(number_immigrants) as sum_immigrants\n",
    "        FROM fact_table\n",
    "        GROUP BY 1,2\n",
    "        ORDER BY 1,2\n",
    "    ) A\n",
    "    WHERE A.sum_immigrants=0\n",
    "\"\"\")\n",
    "immigrants_check_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "**Dimension Tables**\n",
    "```\n",
    "immigrant_df:\n",
    "    ID: id of immigrant\n",
    "    Year: year\n",
    "    Month: month\n",
    "    Origin_country: immigrant's original country\n",
    "    City_code: city port code\n",
    "    State_code: state code of the city\n",
    "    State: name of the state\n",
    "    City: name of the city\n",
    "    \n",
    "demographics_df:\n",
    "    State_code: state code of the city\n",
    "    State: name of the state\n",
    "    Median_age: median age\n",
    "    Percentage_male: male population\n",
    "    Percentage_female: female population\n",
    "    Percentage_veterans: veterans population\n",
    "    Percentage_foreign_born: male population\n",
    "    Percentage_native: male population\n",
    "    Percentage_white: white population\n",
    "    Percentage_afroamerican: afroamerican population\n",
    "    Percentage_latino: latino population\n",
    "    Percentage_asian: asian population\n",
    "    \n",
    "    \n",
    "airport_df:\n",
    "    Ident: ID to identify an airport\n",
    "    Type: Size and type of an airport\n",
    "    Name: The name of an airport \n",
    "    Elevation_ft: The distance above sea level\n",
    "    Country: Country of an airport\n",
    "    State_code: State code of the city an airport is in\n",
    "    City_code: City code that an airport is in\n",
    "    Municipality: The municipality of that city\n",
    "    X_coordinate: Longitude of the airport\n",
    "    Y_coordinate: Latitude of the airport\n",
    "    \n",
    "```\n",
    "\n",
    "**Fact Table**\n",
    "```\n",
    "fact_table:\n",
    "    Year: year\n",
    "    Month: month\n",
    "    Origin_country: immigrant's original country\n",
    "    Number_immigrants: total number of immigrants per state\n",
    "    State_code: state code of the city\n",
    "    State: name of the state\n",
    "    Median_age: median age\n",
    "    Percentage_male: male population\n",
    "    Percentage_female: female population\n",
    "    Percentage_veterans: veterans population\n",
    "    Percentage_foreign_born: male population\n",
    "    Percentage_native: male population\n",
    "    Percentage_white: white population\n",
    "    Percentage_afroamerican: afroamerican population\n",
    "    Percentage_latino: latino population\n",
    "    Percentage_asian: asian population\n",
    "    Airport_name: The name of an airport\n",
    "    X_coordinate: Longitude of the airport\n",
    "    Y_coordinate: Latitude of the airport\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "**Spark** was chosen for this project because of it's capablity of processing Big Data, easily scalable by adding working nodes,and it integrates perfectly with Cloud storages such as AWS S3 and warehouses like AWS Redshift.\n",
    "**Pandas** was chosen here since we're familiar with it and other resources were small compared to .SAS file, and it can be handeled using pandas.\n",
    "\n",
    "- **Propose how often the data should be updated and why**\n",
    "All computed results were grouped by month.\n",
    "So it makes sense to update it monthly.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " - **The data was increased by 100x**\n",
    " Pandas won't be able to handle the load. I will switch to Spark and add more node as needed.\n",
    " \n",
    " - **The data populates a dashboard that must be updated on a daily basis by 7am every day**\n",
    " Apache Airflow is perfect for the job. I will use it to schedule tasks to run everyday.\n",
    " - **The database needed to be accessed by 100+ people**\n",
    " Hosting the database on a cloud based solution will help. That will insure availability to all users and we will pay for what's needed.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
