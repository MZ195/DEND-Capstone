{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project will answer questins related to US immigration.\n",
    "Data sources were provided by Udacity which are:\n",
    "- 94 Immigration Data\n",
    "- U.S. City Demographic Data\n",
    "- Airport Code Table\n",
    "\n",
    "Technologies used are Spark for processing .SAS files.\n",
    "Pandas for processing .csv files\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of this project is to create Dimention and Fact tables from raw data provided from 3 different sources.\n",
    "The final fact table will show the number of immigrants per country and from whcich airport they came and to which state they went.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "- I94 Immigrantion Data: comes form the US National Tourism and Trade Office.\n",
    "- U.S City Demographic Data: comes from OpenSoft.\n",
    "- Airport Code Table: comes from DataHub.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreashold\", \"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read every Dimension table\n",
    "demographicsDF = spark.read.csv(\"us_demographics.csv\",header=True)\n",
    "airportsDF = spark.read.csv(\"us_airports.csv\",header=True)\n",
    "immigrationDF = spark.read.parquet(\"immigration_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For time processing purposes, I will be using only the first 2 months in the immigration dataframe\n",
    "immigrationDF = immigrationDF.filter(f.col(\"month\")<f.lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Dimension TempViews\n",
    "immigrationDF.createOrReplaceTempView(\"immigration\")\n",
    "demographicsDF.createOrReplaceTempView(\"demographics\")\n",
    "airportsDF.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Running SQL Procedure to get the fact table dataframe\n",
    "fact_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        immigration.year,\n",
    "        immigration.month,\n",
    "        immigration.origin_country,\n",
    "        immigration.State,\n",
    "        immigration.state_code,\n",
    "        COUNT(immigration.state_code) as number_immigrants,\n",
    "        demographics.median_age,\n",
    "        demographics.percentage_male,\n",
    "        demographics.percentage_female,\n",
    "        demographics.percentage_veterans,\n",
    "        demographics.percentage_foreign_born,\n",
    "        demographics.Afroamerican as percentage_afroamerican,\n",
    "        demographics.Asian as percentage_asian,\n",
    "        demographics.Latino as percentage_latino,\n",
    "        demographics.Native as percentage_native,\n",
    "        demographics.White as percentage_white,\n",
    "        airports.name as airport_name,\n",
    "        airports.x_coordinate,\n",
    "        airports.y_coordinate\n",
    "    FROM immigration\n",
    "    JOIN demographics ON demographics.state_code=immigration.state_code\n",
    "    JOIN airports ON airports.state_code=immigration.state_code\n",
    "    WHERE airports.type='large_airport'\n",
    "    GROUP BY 1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19\n",
    "    ORDER BY 1,2,3,4\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Count rows in dataframe\n",
    "fact_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Fact Table in Parquet format\n",
    "fact_table.write.mode('overwrite').parquet(\"fact_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "fact_table = spark.read.parquet(\"fact_table\")\n",
    "fact_table.createOrReplaceTempView(\"fact_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check null values per column\n",
    "null_values_test = fact_table.select(f.isnull('year'),\\\n",
    "                  f.isnull('month'),\\\n",
    "                  f.isnull('origin_country'),\\\n",
    "                  f.isnull('state_code'),\\\n",
    "                  f.isnull('State')).dropDuplicates()\n",
    "null_values_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check Sum for number of immigrants equal to 0\n",
    "immigrants_check_sum = spark.sql(\"\"\"\n",
    "    SELECT A.State, A.state_code, A.sum_immigrants\n",
    "    FROM (\n",
    "        SELECT State, state_code, SUM(number_immigrants) as sum_immigrants\n",
    "        FROM fact_table\n",
    "        GROUP BY 1,2\n",
    "        ORDER BY 1,2\n",
    "    ) A\n",
    "    WHERE A.sum_immigrants=0\n",
    "\"\"\")\n",
    "immigrants_check_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check if there are immigrants that were no copied from the original immigrants dimensional table\n",
    "immigrants_check_integrity = spark.sql(\"\"\"\n",
    "    SELECT I.year, I.month, I.State, I.state_code\n",
    "    FROM (\n",
    "        SELECT year, month, State, state_code, COUNT(state_code) as number_immigrants\n",
    "        FROM immigration\n",
    "        GROUP BY 1,2,3,4\n",
    "        ORDER BY 1,2,3,4\n",
    "    ) I, \n",
    "    (\n",
    "        SELECT year, month, State, state_code, SUM(number_immigrants) as sum_immigrants\n",
    "        FROM fact_table\n",
    "        GROUP BY 1,2,3,4\n",
    "        ORDER BY 1,2,3,4\n",
    "    ) F\n",
    "    WHERE I.year=F.year and I.month=F.month and I.state_code=F.state_code and I.number_immigrants>F.sum_immigrants\n",
    "\"\"\")\n",
    "immigrants_check_integrity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
